# [模型配置](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#model-configuration)
模型存储库中的每个模型都必须包含一个模型配置，该配置提供有关模型的必需和可选信息。

## 主要模型配置属性
* platform 
* max_batch_size
* input tensor
* output tensor
* model_transaction_policy

```
platform: "tensorrt_plan"
  max_batch_size: 8   # 模型支持的最大batch大小，对于不支持批处理的模型必须设置为0
  input [
    {
      name: "input0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    },
    {
      name: "input1"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
  output [
    {
      name: "output0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
```
## pytorch后端的约定规则
### 命名规则
1. 输入单纯为张量时：
命名与Torchscript 模型的前向函数定义为 forward(self, input0, input1)参数要同名。
通用命名规则\<name>_<index>.
2. 输入为张量字典时：也就是类似json格式数据作为输入
这样的话，配置命名需要遵守名称name为键名称'A'命名规则。
> {'A': tensor1, 'B': tensor2}

### 配置规则
输入和输出形状由max_batch_size和输入输出的dims维度属性组合指定。  
对于尺寸可变的输入输出模型，可以在配置列表中将其配置为-1.
> dim[4, -1]//可变二维张量输入

## [自动生成模型配置](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#auto-generated-model-configuration)
Triton 可以为大多数 TensorRT、TensorFlow 保存模型和 ONNX 模型自动派生所有必需的设置。
## [张量数据类型](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#datatypes)
Triton 支持的张量数据类型。