# [模型配置](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#model-configuration)
模型存储库中的每个模型都必须包含一个模型配置，该配置提供有关模型的必需和可选信息。

## 主要模型配置属性
* platform 
* max_batch_size
* input tensor
* output tensor
* model_transaction_policy

```
platform: "tensorrt_plan"
  max_batch_size: 8   # 模型支持的最大batch大小，对于不支持批处理的模型必须设置为0
  input [
    {
      name: "input0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    },
    {
      name: "input1"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
  output [
    {
      name: "output0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
```
## pytorch后端的约定规则
### 命名规则
1. 输入单纯为张量时：
命名与Torchscript 模型的前向函数定义为 forward(self, input0, input1)参数要同名。
通用命名规则\<name>_<index>.
2. 输入为张量字典时：也就是类似json格式数据作为输入
这样的话，配置命名需要遵守名称name为键名称'A'命名规则。
> {'A': tensor1, 'B': tensor2}

### 配置规则
输入和输出形状由max_batch_size和输入输出的dims维度属性组合指定。  
对于尺寸可变的输入输出模型，可以在配置列表中将其配置为-1.
> dim[4, -1]//可变二维张量输入

## [自动生成模型配置](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#auto-generated-model-configuration)
Triton 可以为大多数 TensorRT、TensorFlow 保存模型和 ONNX 模型自动派生所有必需的设置。
## [张量数据类型](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#datatypes)
Triton 支持的张量数据类型。

## Instance Groups(实例组)
Triton 可以提供模型的多个实例，已同时处理多个推理请求。在 ModelInstanceGroup 模型配置中设置可执行实例数量和计算资源定义。
默认情况Triton会为每个gpu创建单个执行实例。实例组属性配置可以指定执行GPU位置和数量。
### 例
* 每个GPU实例化两个模型实例
```
instance_group [
    {
      count: 2   # 实例数量
      kind: KIND_GPU  # 执行设备类型也可为CPU
      gpus:[]   # 可以指定GPU，默认不设置为所有GPU
    }
  ]
```
### 主机策略
配置会将实例组设置创建的所有实例与主机策略“policy_0”相关联。
```
instance_group [
    {
      count: 2
      kind: KIND_CPU
      host_policy: "policy_0"
    }
  ]
```
## 模型管理
Triton以三种控制模式之一运行：NONE,EXPLICIT,POLL.其决定了Triton如何处理对模型存储库的更改一家这些协议和API中哪些可用
* None
启动时加载所有模型，这是启动时的默认控制模式。
* EXPLICIT
通过指定启用此模型控制模式 --model-control-mode=explicit，所有模型加载和卸载动作都必须使用模型控制协议显式启动。