# 模型存储库
__定义__: 存储模型的位置，在服务器启动时指定，服务器运行时，可以按照模型管理中的描述修改正在服务的模型。

triton启动时，存储库路径通过--model-repository 选项指定，可以多次指定以包含多个存储库。
> $ tritonserver --model-repository=\<model-repository-path>

## 存储库结构布局
```
  <model-repository-path>/  # 模型仓库根目录
    <model-name>/           # 模型name
      [config.pbtxt]        # 模型配置文件，这几乎是必须的，也可以自动生成
      [<output-labels-file> ...]  # 输出值的标签文件
      <version>/            # 版本号文件夹
        <model-definition-file>  # 模型定义文件
      <version>/
        <model-definition-file>
      ...
    <model-name>/
      [config.pbtxt]
      [<output-labels-file> ...]
      <version>/
        <model-definition-file>
      <version>/
        <model-definition-file>
      ...
    ...
```
* 每个子目录至少有一个代表模型版本的数字子目录
* 不同的模型有特定的后端执行，需要提供特定于框架的模型文件

## 存储位置
* 本地文件系统
指定绝对路径
> $ tritonserver --model-repository=/path/to/model/repository ...
* 云存储  
> $ tritonserver --model-repository=gs://bucket/path/to/model/repository ...

# [模型配置](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#model-configuration)
模型存储库中的每个模型都必须包含一个模型配置，该配置提供有关模型的必需和可选信息。

## 模型基本配置属性
* platform 
* max_batch_size
* input tensor
* output tensor
* model_transaction_policy

```
platform: "tensorrt_plan"
  max_batch_size: 8   # 模型支持的最大batch大小，对于不支持批处理的模型必须设置为0
  input [
    {
      name: "input0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    },
    {
      name: "input1"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
  output [
    {
      name: "output0"
      data_type: TYPE_FP32
      dims: [ 16 ]
    }
  ]
```


## Instance Groups(实例组)
Triton 可以提供模型的多个实例，已同时处理多个推理请求。在 ModelInstanceGroup 模型配置中设置可执行实例数量和计算资源定义。
默认情况Triton会为每个gpu创建单个执行实例。实例组属性配置可以指定执行GPU位置和数量。
### 例
* 每个GPU实例化两个模型实例
```
instance_group [
    {
      count: 2   # 实例数量
      kind: KIND_GPU  # 执行设备类型也可为CPU
      gpus:[]   # 可以指定GPU，默认不设置为所有GPU
    }
  ]
```

## 模型类型及调度器配置
* 无状态模型
各模型推理相互独立执行，默认调度器，一般无中间结果存储和跨模型推理请求（）。
* 有状态模型
多个推理请求构成一个序列，使用同一模型实例进行推理，要保证模型状态能够被正确更新。
* 集成模型
一个集成模型可以由一个或多个输出输出由连接关系的模型构成，通常用于封装一个多模型的推理过程<数据预处理->推理->数据后处理>。
可以减少中间过程的张量传输和请求量。
### 集成模型的配置
集成调度器（ensemble scheduler）用于集成模型，其规定了数据流向（ModelEnsembling::Step）、输入要求及包含模型的所涉及的属性。

如下一个集成了图像分割和图像分类的模型配置
```
name: "ensemble_model"
platform: "ensemble"
max_batch_size: 1
input [
  {
    name: "IMAGE"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]
output [
  {
    name: "CLASSIFICATION"
    data_type: TYPE_FP32
    dims: [ 1000 ]
  },
  {
    name: "SEGMENTATION"
    data_type: TYPE_FP32
    dims: [ 3, 224, 224 ]
  }
]
//以上是模型常规配置
ensemble_scheduling {  //集成调度器配置：step集成了图像预处理、图像分割、图像分类，通过input_map、output_map映射数据，并指定数据流向。
  step [
    {
      model_name: "image_preprocess_model"
      model_version: -1  //最新版本
      input_map {
        key: "RAW_IMAGE"  //数据映射
        value: "IMAGE"
      }
      output_map {
        key: "PREPROCESSED_OUTPUT"
        value: "preprocessed_image"   //输入为预处理后的图像
      }
    },
    {
      model_name: "classification_model"
      model_version: -1
      input_map {
        key: "FORMATTED_IMAGE"
        value: "preprocessed_image"  //作为输入
      }
      output_map {
        key: "CLASSIFICATION_OUTPUT"
        value: "CLASSIFICATION"
      }
    },
    {
      model_name: "segmentation_model"
      model_version: -1
      input_map {
        key: "FORMATTED_IMAGE"
        value: "preprocessed_image"  //作为输入
      }
      output_map {
        key: "SEGMENTATION_OUTPUT"
        value: "SEGMENTATION"
      }
    }
  ]
}
```
#### 集成调度器接收并响应推理请求流程
1. request中获取"IMAGE"tensor并映射至输入"RAW_IMAGE"
2. 向预处理模型发送内部请求
3. 预处理模型进行推理和数据映射
4. 此时，分类模型和分割模型均标记为就绪
5. 发送内部请求进行就绪模型推理计算
6. 将输出结果响应request


## 模型管理

### 版本
不以数字命名的版本子目录会被忽略。

### 子目录模型文件
取决于模型类型和支持的后端决定
* TensorRT 模型
.plan 文件，默认命名为model.plan.Tensorrt模型要将plan模型与当前硬件的计算能力关联，需要在模型配置文件设置cc_model_filenames属性。
模型仓库最小构成
```
<model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.plan
```
* TorchScript 模型
模型仓库最小构成
```
<model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.pt
```
* python 模型
Python 后端允许您在 Triton 中将 Python 代码作为模型运行。默认情况下，Python 脚本必须命名为 model.py.
```
<model-repository-path>/
    <model-name>/
      config.pbtxt
      1/
        model.py
```

## [模型配置文件设置及优化](https://github.com/triton-inference-server/server/blob/main/docs/model_configuration.md#instance-groups)
Triton以三种控制模式之一运行：NONE,EXPLICIT,POLL.其决定了Triton如何处理对模型存储库的更改一家这些协议和API中哪些可用
* None
启动时加载所有模型，这是启动时的默认控制模式。
* EXPLICIT
通过指定启用此模型控制模式 --model-control-mode=explicit，所有模型加载和卸载动作都必须使用模型控制协议显式启动。
### 模型预热
默认情况模型只有收到相应的第一个推理请求时，Triton相对应的后端才会初始化它，这样第一个推理请求的执行就会明显变慢，为避免这一情况，可以在模型配置文件中定义modelWarmup属性，设置模型预热。。这些设置定义了 Triton 将创建的一系列推理请求，以预热每个模型实例。仅当模型实例成功完成请求时才会提供服务。
