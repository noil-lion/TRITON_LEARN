
# Triton server响应调度配置指南
在介绍server的推理请求处理调度前，先明确一下server端管理的模型类型，这决定了server端要使用不用的策略来管理不同模型类型的推理请求。是triton server中的模型的推理请求响应优化的基础理论。  
简单来说有三种类型，无状态型模型、有状态模型、集成模型。
有无状态比较不好理解，通俗来区分就是：假定现在有两个推理请求前后发送到一个模型进行推理，如果第二个推理请求需要第一个推理请求结束后的数据或者模型状态，也就是两个请求前后有关联，多个推理请求一起形成一系列推理，且要路由到同一个模型实例进行推理计算，这样进行推理请求响应的模型被称为有状态模型，而无状态模型前后的推理请求互相独立，无需将两个请求路由到同一个模型实例，甚至可以一个模型实例响应一个请求，这样的模型就是无需状态维护的模型。  
本文简述最常见的无状态模型的triton处理调度策略，关于有状态模型和集成模型的处理调度，参考[文档详情](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#stateful-models)
# 批处理和调度
triton server对单个推理请求的一批输入的批量推理处理可大大提高服务批处理的吞吐量。推理服务器自带了多种调度和批处理算法，用于支持大量不同的模型类型和实例。
Triton中可选动态批处理器、序列批处理器、集成调度器。其中，默认调度器或动态批处理器可以用作这些无状态模型的调度器，序列批处理器常用于有状态模型，集成调度器用于集成模型的推理请求管理。


## 默认调度
默认就是将所有推理请求分发给对应的模型所有实例

## 动态批处理
动态批处理是triton的一种特性，这使得推理请求可以通过结合服务器端动态地创建批请求，通常会导致吞吐量增加，动态批处理器用于无状态模型，其创建的批请求会被分发到模型配置中的所有模型实例中。  
默认情况下，动态批处理器维护一个队列，其中包含模型的所有推理请求。请求按顺序进行处理和批处理。  
### 动态批处理器使用
通过在模型配置文件中对ModelDynamicBatching属性进行设置对每个模型单独启用并配置各自的动态批处理器，通过配置可设置1. 动态创建的批处理的首选大小 2. 请求可以在调度程序中的延迟时间(这将决定其它请求加入动态批的最长时间) 3. 批队列的相关属性管理（队列大小、优先级-priority_levels-、延时-delay-）

还可以通过模型分析器进行自动搜索不同的动态批处理器配置。

### 动态批处理器实例配置流程
1. 确定模型的请求的最大批量大小(maximum batch size)。
2. 添加动态批处理器默认配置项，创建尽可能大的批次，直到最大批次大小，并且在形成批次时不会延迟。
```
 dynamic_batching { }
```
3. 性能评估：使用性能分析器确定默认动态批处理器配置提供的延迟和吞吐量。
4. 如果默认配置的延迟是可以接受的，那就可以适当增加吞吐量，达到性能平衡。
* 增加最大批量大小。
* 将批处理延迟（batch——delay）设置为非零值。尝试增加延迟值直到超过延迟预算以查看对吞吐量的影响。
5. 通常不会直接配置Preferred batch sizes， 除非已经验证这一参数是明显优越的。

### 动态批处理器执行流程
1. 服务启动，模型加载并处于ready状态
2. 动态批处理器根据调度程序中收到的推理请求，顺序添加推理请求创建批处理。
3. 当推理请求的数量无法构建形成首选大小的批处理任务时，将会发送小于小于模型允许的批处理大小的批处理任务。
4. 第三步的具体控制涉及延迟批处理，只要推理请求的到达时间没有超过配置的延迟时间，动态批处理器就会延迟发送此批处理
5. 当请求总数达到能构成的最大批时，就会直接发送。同时，超过延迟时间，也会将当前构造好的批发送。
注：生成的batches的具体数目可以使用count metrics进行汇总监测。
动态批处理器可以配置为允许请求在调度器中延迟有限的时间，以允许其他请求加入动态批处理。以例 max_queue_delay_microseconds: 100，是指延迟100微秒。
### 批队列的控制策略
控制推理请求如何排队等待批处理  
_ModelQueuePolicy_属性中可设置max_queue_size, timeout_action, default_timeout_microseconds and allow_timeout_override.


## [序列批处理器](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#stateful-models)
使用模型配置中的ModelSequenceBatching属性为每个模型单独启用和配置序列批处理。这些设置控制序列超时以及配置 Triton 如何将控制信号发送到模型，指示序列开始、结束、就绪和相关 ID。
更多细节待补充

## [集成调度器](https://github.com/triton-inference-server/server/blob/main/docs/architecture.md#ensemble-models)
使用模型配置中的ModelEnsembleScheduling属性为每个模型单独启用和配置集成调度程序。这些设置描述了集成中包含的模型以及模型之间的张量值流。

### 模型配置优化
模型配置ModelOptimizationPolicy属性用于指定模型的优化和优先级设置。这些设置控制模型是否/如何由后端优化，以及如何由 Triton 调度和执行。
### 模型预热
Triton 提供了一个配置选项，使模型能够“预热”，以便在收到第一个推理请求之前完全初始化它。在模型配置中定义ModelWarmup属性时，Triton 会在预热好模型后将模型显示为ready。
