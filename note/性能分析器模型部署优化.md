# 配置优化
Triton 推理服务器具有许多功能，可用于减少延迟并增加模型的吞吐量。  
基于的实例是基于包括客户端、服务端及模型存储库，核心在于介绍单个模型的延迟和吞吐量权衡,主要基于airway模型作为基线，使用Perf_analuzer来确定合理的模型配置用于实际部署。


## 实例构建
__实例一__. 不启用任何性能优化的基本模型配置
```
perf_analyzer -m airway --percentile=95 --concurrency-range 1:4
//使用PA性能分析器对airway模型进行多层级并发请求性能分析
//结果
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 10.9 infer/sec, latency 137090 usec
Concurrency: 2, throughput: 14.5 infer/sec, latency 255302 usec
Concurrency: 3, throughput: 17.7 infer/sec, latency 223294 usec
Concurrency: 4, throughput: 17.4 infer/sec, latency 312755 usec

//非优化模型配置提供了大约每秒17次推理的吞吐量，延迟在高并发情况下增加。
```
__在并发请求数为2时，吞吐量有一定增加，但后面再增加并发数，吞吐量无明显增加,这是由于两个请求通信重叠，这足以完全隐藏通信延迟，也就是不让Triton server有因为请求通信而产生空闲的时间__

__实例二__. 动态批处理
动态批处理器将单个的推理请求进行组合，构建一个比单独执行更具效率的批处理任务。要启用可在配置文件末尾添加dynamic_batching{}
```
//配置文件添加
dynamic_batching { }

//使用PA性能分析器对开启动态批处理器的airway模型进行多层级并发请求性能分析
//结果
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 12.1 infer/sec, latency 136404 usec
Concurrency: 2, throughput: 17.6 infer/sec, latency 164176 usec
Concurrency: 3, throughput: 19.8 infer/sec, latency 159624 usec
Concurrency: 4, throughput: 20.0333 infer/sec, latency 209223 usec

//启用动态批处理配置提供了大约每秒20次推理的吞吐量，且平均延迟降低
```

__为尽可能得到最小延时，可以将并发数量=1，模型实1，禁止使用动态批处理器__  
__为尽可能获得最大地吞吐量，可以将并发数设置为[2 * maximum batch size * model instance count]，可达到吞吐量最大化__

__实例三__. Model Instances  
Triton允许指定每个模型有多少个副本用于推理，可在模型配置中指定任意数量，理论上多个实例会提高性能，因为这样可以内存数据传输与推理计算时间重叠，还有多实例并行推理，小模型在多实例上会有更好效果。
```
//配置文件添加
instance_group [
    {
      count: 2
      kind: KIND_GPU
    }
```
```
perf_analyzer -m airway --percentile=95 --concurrency-range 1:4
//使用PA性能分析器对基于count=2时的airway模型并发请求测试结果
//结果
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 11.4333 infer/sec, latency 104353 usec
Concurrency: 2, throughput: 21.6333 infer/sec, latency 162145 usec
Concurrency: 3, throughput: 21.8667 infer/sec, latency 256913 usec
Concurrency: 4, throughput: 21.6 infer/sec, latency 289096 usec
Concurrency: 5, throughput: 22.9667 infer/sec, latency 319899 usec
Concurrency: 6, throughput: 25.5333 infer/sec, latency 312346 usec

//启用多实例配置提供了大约每秒25次推理的最大吞吐量，平均延迟与未进行配置优化相近，但却支持了更高的并发量和吞吐量。
```

__实例四__. 动态批处理器+模型实例

```
Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 7.9 infer/sec, latency 151986 usec
Concurrency: 2, throughput: 8 infer/sec, latency 158022 usec
Concurrency: 3, throughput: 11.4 infer/sec, latency 291932 usec
Concurrency: 4, throughput: 12.7 infer/sec, latency 401315 usec
```
__动态批和多模型实例地结合并不一定就会提高服务性能，通常这两者的性能提升都是特定于模型的，比如在单实例已经能充分使用当前GPU，再添加一个实例并不会提供任何性能优势__

__模型配置优化特定于模型，其优化需根据模型和实际的部署环境进行调整，最终的目标是确定的，在尽可能大吞吐量的情况下，限制在可接受的延迟__