# Triton的性能分析器程序 | performance_analyzer
尝试不同的优化策略时测量性能变化，perf_analyzer包含在client中，可通过启动client镜像进行使用。

## 性能分析器基本概念
PA为指定模型生成推理请求并测量推理请求的吞吐量和延迟，分析模型部署的服务性能。  
具体实现，通过重复测量等时间窗口间隔内的吞吐量和延迟，直到结果稳定，通常使用平均延迟来确定结果的稳定性，也可以使用百分位数来作为结果是否稳定的置信度指标，一般使用95百分位的数值作为衡量结果。
默认Triton使用最低负载来测量模型的延迟和吞吐量，默认PA使用请求并发数量1，收到响应后立即发送另一个请求，在测量窗口时间内重复此过程，perf_analyzer 报告包括客户端看到的延迟和吞吐量以及服务器的平均请求延迟。    

__服务器延迟（request latency）__: 测量从服务器收到请求到服务器发送响应的总时间。
* 排队延迟queue：推理请求在调度队列里等模型实例的平均时间。
* 计算延迟（input、infer、output）：执行推理所话费的平均时间，包括数据从GPU与CPU的数据传输时间。

__客户端延迟（HTTP/GRPC time）__:
* HTTP（send/recv+response）：表示客户端发送请求和接收响应所花费的时间，response wait表示等待服务器响应的时间。
* GRPC:(un)marshal request/response表示将请求数据编组到 GRPC protobuf 以及从 GRPC protobuf 中解组响应数据所花费的时间。response wait表示将 GRPC 请求写入网络、等待响应以及从网络读取 GRPC 响应的时间。

## 测试结果记录
perf_analyzer 提供了 -f 选项来生成包含结果的 CSV 输出的文件。

## 输入数据及配置选项
1. 默认情况PA将生成随机数据并发送到模型的输入中。可以使用配置选项--input-data进行选择输入输入模式
* random：默认随机。
* zero：输入都为0。
* 数据路径：输入数据的原始二进制文件目录的路径。
* 文件路径：包含要用于每个推理请求的数据的 JSON 文件的路径。
注: __对于具有 STRING/BYTES 数据类型的张量，在某些情况下可能会使用额外的选项 --string-length 和 --string-data__
2. 对于支持批处理的模型，可以使用-b选项来指示PA应该发送的请求的批处理大小
3. 对于具有可变大小输入的模型，要提供 --shape 参数

__对于高度依赖数据的模型，可以在json文件中提供要使用的真实数据，以便PA在发送推理请求时一循环顺序使用__

## 通讯协议
默认下，PA以HTTP和Triton通信，GRPC协议可以以-i选项指定。

## 测试分析实操
启动客户端容器，此时会进入客户端容器内部的workspace文件夹，可在此进行发起请求和性能分析。
```
docker run -it --rm --net=host nvcr.io/nvidia/tritonserver:22.04-py3-sdk
```

__简单的模型性能测试__
```
perf_analyzer -m inception_graphdef --percentile=95
//测量设置
Batch size: 1  //BS=1
Measurement window: 5000 msec  //时间窗5000微秒
Using synchronous calls for inference  //使用异步调用推理请求
Stabilizing using p95 latency  //稳定指标默认为95百分位数

//测量结果
Request concurrency: 1  //请求并发量，可以使用--concurrency-range <start>:<end>:<step>进行设置多种并发请求的情况
  Client: 
    Request count: 60    //请求数
    Throughput: 12 infer/sec   //  平均推理次数每秒
    p50 latency: 77119 usec
    p90 latency: 110080 usec
    p95 latency: 121369 usec
    p99 latency: 127726 usec  //请求响应延迟时间99百分位数值
    Avg HTTP time: 82194 usec (send/recv 23743 usec + response wait 58451 usec)  //客户端延迟
  Server: 
    Inference count: 73
    Execution count: 73
    Successful request count: 73
    Avg request latency: 51382 usec (overhead 57 usec + queue 83 usec + compute input 1970 usec + compute infer 6385 usec + compute output 42887 usec)  //服务器延迟

Inferences/Second vs. Client p95 Batch Latency
Concurrency: 1, throughput: 12 infer/sec, latency 121369 usec
```
