# 部署方案概述
模型部署方案
## 基本路线
1. pytorch->torchScript->pytorch_backend->triton

2. pytorch->torchScript->onnx->tensorrt.engine->tensorrt_backend->triton

通过保存可独立于Python环境的Pytorch训练结束的模型，
可通过模型压缩、加速、移植将其调整至可运行至工作环境中，
将模型封装成一个SDK、web服务，对外暴露访问接口，实现模型的服务化。

模型训练->模型推理->模型部署->模型服务化

Triton 支持TorchScript模型文件。

## pytorch模型保存之Torchscript
TorchScript是一种从PyTorch代码创建可序列化和可优化模型的方法。任何TorchScript都可以从Python程序中保存，并在没有Python环境的程序中加载。便于在生成环境（C++）中导出。

* 使用torch.save()保存的模型文件，无论是保存的模型参数还是模型本身，想要load的话，必须拥有正确的python环境和模型定义。因为torch.save()本质上是pickle操作，仅仅是把实例对象内容序列化存储到磁盘上而已，反序列化回来的时候需要知道数据对象的类定义，所以不具备独立python环境记录数据的可能性。

* TorchScript格式的模型，必须通过torch.jit.load()方法加载，如果用torch.load()加载则会抛出异常。
```torchScript模型保存实例
# 模型保存
# @model：PyTorch网络模型实例化对象
# @dummpy_input: PyTorch模型的输入Tensor
# @model_name：PyTorch模型名称
script_model = torch.jit.trace(model, dummpy_input)
torch.jit.save(script_model, "{}.pth".format(model_name))
​
# 模型加载
# @model_name：PyTorch模型名称
script_model = torch.jit.load("{}.pth".format(model_name))
```
##  模型转换
### Torch->onnx
* [torch2onnx](https://pypi.org/project/torch2onnx/)
还有很多细节后期补充
### Torch->TorchScript
未知pytorch模型文件如何转为TorchScript模型
1. 使用torch.load()尝试加载文件，如果抛出异常，则可以尝试TorchScrpit加载方式加载文件，调用torch.jit.trace()过一遍模型的实例化，调用torch.jit.save()得到模型的TorchScript文件。
2. 如果Torch.load()加载模型成功，则需要提供网络定义的python文件和类名，判断torch.load()得到的对象的类型，如果是collections.OrderedDict，那么则是StateDict，用python的反射机制实例化网络定义类成模型model，调用model.load_state_dict()加载完整模型，调用torch.jit.trace()过一遍模型的实例化即可，最后调用torch.jit.save()就得到了TorchScript。
# 推理引擎构建
## backend推理引擎构建-Tensorrt_backend构建
从推理引擎层面对模型推理进行加速，从而达到模型推理性能调整，细节代补充。
## backend推理引擎构建-Pytorch_backend构建




